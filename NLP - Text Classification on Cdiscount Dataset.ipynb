{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Comparing Classaification Models in an NLP task (Text Classification) on Cdiscount Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "\n",
    "%matplotlib inline\n",
    "import pylab as plt\n",
    "\n",
    "pd.set_option('display.max_columns', 100)\n",
    "pd.set_option('display.max_columns', 200)\n",
    "\n",
    "pd.options.display.encoding = 'ascii'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We will use Models from the Scikit-Learn Machine Learning Librarie "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le naive bayes multinomial classifieur est approprié pour la classification avec des features discretes (comptage des mots pour la classification de texte).\n",
    "La distribution multinomiale nécessite habituellement des nombres entiers.  \n",
    "Cependant, en pratique on peut également faire des counts tel que tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Train, Test and Score Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainAndScore(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    score = model.score(X_test, y_test)\n",
    "    print('SCORE =  ',score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Pre-Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stopwords Français"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopWords = [\"a\",\"abord\",\"absolument\",\"afin\",\"ah\",\"ai\",\"aie\",\"aient\",\"aies\",\"ailleurs\",\"ainsi\",\"ait\",\"allaient\",\"allo\",\"allons\",\"allô\",\"alors\",\"anterieur\",\"anterieure\",\"anterieures\",\"apres\",\"après\",\"as\",\"assez\",\"attendu\",\"au\",\"aucun\",\"aucune\",\"aucuns\",\"aujourd\",\"aujourd'hui\",\"aupres\",\"auquel\",\"aura\",\"aurai\",\"auraient\",\"aurais\",\"aurait\",\"auras\",\"aurez\",\"auriez\",\"aurions\",\"aurons\",\"auront\",\"aussi\",\"autre\",\"autrefois\",\"autrement\",\"autres\",\"autrui\",\"aux\",\"auxquelles\",\"auxquels\",\"avaient\",\"avais\",\"avait\",\"avant\",\"avec\",\"avez\",\"aviez\",\"avions\",\"avoir\",\"avons\",\"ayant\",\"ayez\",\"ayons\",\"b\",\"bah\",\"bas\",\"basee\",\"bat\",\"beau\",\"beaucoup\",\"bien\",\"bigre\",\"bon\",\"boum\",\"bravo\",\"brrr\",\"c\",\"car\",\"ce\",\"ceci\",\"cela\",\"celle\",\"celle-ci\",\"celle-là\",\"celles\",\"celles-ci\",\"celles-là\",\"celui\",\"celui-ci\",\"celui-là\",\"celà\",\"cent\",\"cependant\",\"certain\",\"certaine\",\"certaines\",\"certains\",\"certes\",\"ces\",\"cet\",\"cette\",\"ceux\",\"ceux-ci\",\"ceux-là\",\"chacun\",\"chacune\",\"chaque\",\"cher\",\"chers\",\"chez\",\"chiche\",\"chut\",\"chère\",\"chères\",\"ci\",\"cinq\",\"cinquantaine\",\"cinquante\",\"cinquantième\",\"cinquième\",\"clac\",\"clic\",\"combien\",\"comme\",\"comment\",\"comparable\",\"comparables\",\"compris\",\"concernant\",\"contre\",\"couic\",\"crac\",\"d\",\"da\",\"dans\",\"de\",\"debout\",\"dedans\",\"dehors\",\"deja\",\"delà\",\"depuis\",\"dernier\",\"derniere\",\"derriere\",\"derrière\",\"des\",\"desormais\",\"desquelles\",\"desquels\",\"dessous\",\"dessus\",\"deux\",\"deuxième\",\"deuxièmement\",\"devant\",\"devers\",\"devra\",\"devrait\",\"different\",\"differentes\",\"differents\",\"différent\",\"différente\",\"différentes\",\"différents\",\"dire\",\"directe\",\"directement\",\"dit\",\"dite\",\"dits\",\"divers\",\"diverse\",\"diverses\",\"dix\",\"dix-huit\",\"dix-neuf\",\"dix-sept\",\"dixième\",\"doit\",\"doivent\",\"donc\",\"dont\",\"dos\",\"douze\",\"douzième\",\"dring\",\"droite\",\"du\",\"duquel\",\"durant\",\"dès\",\"début\",\"désormais\",\"e\",\"effet\",\"egale\",\"egalement\",\"egales\",\"eh\",\"elle\",\"elle-même\",\"elles\",\"elles-mêmes\",\"en\",\"encore\",\"enfin\",\"entre\",\"envers\",\"environ\",\"es\",\"essai\",\"est\",\"et\",\"etant\",\"etc\",\"etre\",\"eu\",\"eue\",\"eues\",\"euh\",\"eurent\",\"eus\",\"eusse\",\"eussent\",\"eusses\",\"eussiez\",\"eussions\",\"eut\",\"eux\",\"eux-mêmes\",\"exactement\",\"excepté\",\"extenso\",\"exterieur\",\"eûmes\",\"eût\",\"eûtes\",\"f\",\"fais\",\"faisaient\",\"faisant\",\"fait\",\"faites\",\"façon\",\"feront\",\"fi\",\"flac\",\"floc\",\"fois\",\"font\",\"force\",\"furent\",\"fus\",\"fusse\",\"fussent\",\"fusses\",\"fussiez\",\"fussions\",\"fut\",\"fûmes\",\"fût\",\"fûtes\",\"g\",\"gens\",\"h\",\"ha\",\"haut\",\"hein\",\"hem\",\"hep\",\"hi\",\"ho\",\"holà\",\"hop\",\"hormis\",\"hors\",\"hou\",\"houp\",\"hue\",\"hui\",\"huit\",\"huitième\",\"hum\",\"hurrah\",\"hé\",\"hélas\",\"i\",\"ici\",\"il\",\"ils\",\"importe\",\"j\",\"je\",\"jusqu\",\"jusque\",\"juste\",\"k\",\"l\",\"la\",\"laisser\",\"laquelle\",\"las\",\"le\",\"lequel\",\"les\",\"lesquelles\",\"lesquels\",\"leur\",\"leurs\",\"longtemps\",\"lors\",\"lorsque\",\"lui\",\"lui-meme\",\"lui-même\",\"là\",\"lès\",\"m\",\"ma\",\"maint\",\"maintenant\",\"mais\",\"malgre\",\"malgré\",\"maximale\",\"me\",\"meme\",\"memes\",\"merci\",\"mes\",\"mien\",\"mienne\",\"miennes\",\"miens\",\"mille\",\"mince\",\"mine\",\"minimale\",\"moi\",\"moi-meme\",\"moi-même\",\"moindres\",\"moins\",\"mon\",\"mot\",\"moyennant\",\"multiple\",\"multiples\",\"même\",\"mêmes\",\"n\",\"na\",\"naturel\",\"naturelle\",\"naturelles\",\"ne\",\"neanmoins\",\"necessaire\",\"necessairement\",\"neuf\",\"neuvième\",\"ni\",\"nombreuses\",\"nombreux\",\"nommés\",\"non\",\"nos\",\"notamment\",\"notre\",\"nous\",\"nous-mêmes\",\"nouveau\",\"nouveaux\",\"nul\",\"néanmoins\",\"nôtre\",\"nôtres\",\"o\",\"oh\",\"ohé\",\"ollé\",\"olé\",\"on\",\"ont\",\"onze\",\"onzième\",\"ore\",\"ou\",\"ouf\",\"ouias\",\"oust\",\"ouste\",\"outre\",\"ouvert\",\"ouverte\",\"ouverts\",\"o|\",\"où\",\"p\",\"paf\",\"pan\",\"par\",\"parce\",\"parfois\",\"parle\",\"parlent\",\"parler\",\"parmi\",\"parole\",\"parseme\",\"partant\",\"particulier\",\"particulière\",\"particulièrement\",\"pas\",\"passé\",\"pendant\",\"pense\",\"permet\",\"personne\",\"personnes\",\"peu\",\"peut\",\"peuvent\",\"peux\",\"pff\",\"pfft\",\"pfut\",\"pif\",\"pire\",\"pièce\",\"plein\",\"plouf\",\"plupart\",\"plus\",\"plusieurs\",\"plutôt\",\"possessif\",\"possessifs\",\"possible\",\"possibles\",\"pouah\",\"pour\",\"pourquoi\",\"pourrais\",\"pourrait\",\"pouvait\",\"prealable\",\"precisement\",\"premier\",\"première\",\"premièrement\",\"pres\",\"probable\",\"probante\",\"procedant\",\"proche\",\"près\",\"psitt\",\"pu\",\"puis\",\"puisque\",\"pur\",\"pure\",\"q\",\"qu\",\"quand\",\"quant\",\"quant-à-soi\",\"quanta\",\"quarante\",\"quatorze\",\"quatre\",\"quatre-vingt\",\"quatrième\",\"quatrièmement\",\"que\",\"quel\",\"quelconque\",\"quelle\",\"quelles\",\"quelqu'un\",\"quelque\",\"quelques\",\"quels\",\"qui\",\"quiconque\",\"quinze\",\"quoi\",\"quoique\",\"r\",\"rare\",\"rarement\",\"rares\",\"relative\",\"relativement\",\"remarquable\",\"rend\",\"rendre\",\"restant\",\"reste\",\"restent\",\"restrictif\",\"retour\",\"revoici\",\"revoilà\",\"rien\",\"s\",\"sa\",\"sacrebleu\",\"sait\",\"sans\",\"sapristi\",\"sauf\",\"se\",\"sein\",\"seize\",\"selon\",\"semblable\",\"semblaient\",\"semble\",\"semblent\",\"sent\",\"sept\",\"septième\",\"sera\",\"serai\",\"seraient\",\"serais\",\"serait\",\"seras\",\"serez\",\"seriez\",\"serions\",\"serons\",\"seront\",\"ses\",\"seul\",\"seule\",\"seulement\",\"si\",\"sien\",\"sienne\",\"siennes\",\"siens\",\"sinon\",\"six\",\"sixième\",\"soi\",\"soi-même\",\"soient\",\"sois\",\"soit\",\"soixante\",\"sommes\",\"son\",\"sont\",\"sous\",\"souvent\",\"soyez\",\"soyons\",\"specifique\",\"specifiques\",\"speculatif\",\"stop\",\"strictement\",\"subtiles\",\"suffisant\",\"suffisante\",\"suffit\",\"suis\",\"suit\",\"suivant\",\"suivante\",\"suivantes\",\"suivants\",\"suivre\",\"sujet\",\"superpose\",\"sur\",\"surtout\",\"t\",\"ta\",\"tac\",\"tandis\",\"tant\",\"tardive\",\"te\",\"tel\",\"telle\",\"tellement\",\"telles\",\"tels\",\"tenant\",\"tend\",\"tenir\",\"tente\",\"tes\",\"tic\",\"tien\",\"tienne\",\"tiennes\",\"tiens\",\"toc\",\"toi\",\"toi-même\",\"ton\",\"touchant\",\"toujours\",\"tous\",\"tout\",\"toute\",\"toutefois\",\"toutes\",\"treize\",\"trente\",\"tres\",\"trois\",\"troisième\",\"troisièmement\",\"trop\",\"très\",\"tsoin\",\"tsouin\",\"tu\",\"té\",\"u\",\"un\",\"une\",\"unes\",\"uniformement\",\"unique\",\"uniques\",\"uns\",\"v\",\"va\",\"vais\",\"valeur\",\"vas\",\"vers\",\"via\",\"vif\",\"vifs\",\"vingt\",\"vivat\",\"vive\",\"vives\",\"vlan\",\"voici\",\"voie\",\"voient\",\"voilà\",\"vont\",\"vos\",\"votre\",\"vous\",\"vous-mêmes\",\"vu\",\"vé\",\"vôtre\",\"vôtres\",\"w\",\"x\",\"y\",\"z\",\"zut\",\"à\",\"â\",\"ça\",\"ès\",\"étaient\",\"étais\",\"était\",\"étant\",\"état\",\"étiez\",\"étions\",\"été\",\"étée\",\"étées\",\"étés\",\"êtes\",\"être\",\"ô\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "689"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegexpTokenizer(pattern='[a-zA-Z0-9]+', gaps=False, discard_empty=True, flags=<RegexFlag.UNICODE|DOTALL|MULTILINE: 56>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "token = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To replace NaN Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replaceNaNValues(df, columnName) :\n",
    "    print('------ BEFORE REPLACING : ', df[columnName].isna().sum(), ' NAN VALUES')\n",
    "    x = train[columnName].mode()\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        if pd.isna(row[columnName]) :\n",
    "            df.loc[index, columnName] = x[0]\n",
    "    \n",
    "    print('------ AFTER : ', df[columnName].isna().sum(), ' NAN VALUES')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Testing Data Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizeData(vectorizer, dfTrain, dfTest, Feature, Target) :\n",
    "    X_train = vectorizer.fit_transform(dfTrain[Feature])\n",
    "    y_train = dfTrain[Target]\n",
    "\n",
    "    X_test = vectorizer.transform(dfTest[Feature])\n",
    "    y_test = dfTest[Target]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('data/train.csv', index_col=0)\n",
    "test = pd.read_csv('data/test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the Datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((20362, 6), (6788, 6))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Libelle</th>\n",
       "      <th>Marque</th>\n",
       "      <th>prix</th>\n",
       "      <th>Categorie3</th>\n",
       "      <th>Categorie3_Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20189</td>\n",
       "      <td>Le centenaire de la Grande Guerre - Jean Huon</td>\n",
       "      <td>14-18 La guerre totale</td>\n",
       "      <td>AUCUNE</td>\n",
       "      <td>24.90</td>\n",
       "      <td>1000014152</td>\n",
       "      <td>HISTOIRE DE FRANCE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6928</td>\n",
       "      <td>Bouilloire sans fil - Puissance:  2200 W - Cap...</td>\n",
       "      <td>Bouilloire en verre / Gris - Art &amp; Cuisine BV22G</td>\n",
       "      <td>A&amp;C Home</td>\n",
       "      <td>21500.00</td>\n",
       "      <td>1000003662</td>\n",
       "      <td>BOUILLOIRE ELECTRIQUE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3422</td>\n",
       "      <td>LIVRAISON INCLUSE Portique métal/bois 2,30 m 3...</td>\n",
       "      <td>Portique bois/métal 2,30 m 3 agres + 1 PF+ tipi</td>\n",
       "      <td>TRIGANO</td>\n",
       "      <td>356.99</td>\n",
       "      <td>1000002072</td>\n",
       "      <td>BALANCOIRE - PORTIQUE - STATION DE JEUX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26260</td>\n",
       "      <td>Lit complet composé d'1 cadre de lit, de 2 mat...</td>\n",
       "      <td>Lit complet avec coffre Gris 140x190cm Cosmo</td>\n",
       "      <td>AUCUNE</td>\n",
       "      <td>281100.00</td>\n",
       "      <td>1000015782</td>\n",
       "      <td>LIT COMPLET - MEUBLE ET SOMMIER - MEUBLE ET SO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1569</td>\n",
       "      <td>Adhésif PVC toile orange 75X33</td>\n",
       "      <td>Adhésif PVC toile orange 75X33</td>\n",
       "      <td>AUCUNE</td>\n",
       "      <td>4.80</td>\n",
       "      <td>1000001535</td>\n",
       "      <td>RUBAN ADHESIF - ADHESIF PALETTISATION</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Description  \\\n",
       "20189      Le centenaire de la Grande Guerre - Jean Huon   \n",
       "6928   Bouilloire sans fil - Puissance:  2200 W - Cap...   \n",
       "3422   LIVRAISON INCLUSE Portique métal/bois 2,30 m 3...   \n",
       "26260  Lit complet composé d'1 cadre de lit, de 2 mat...   \n",
       "1569                      Adhésif PVC toile orange 75X33   \n",
       "\n",
       "                                                Libelle    Marque       prix  \\\n",
       "20189                            14-18 La guerre totale    AUCUNE      24.90   \n",
       "6928   Bouilloire en verre / Gris - Art & Cuisine BV22G  A&C Home   21500.00   \n",
       "3422    Portique bois/métal 2,30 m 3 agres + 1 PF+ tipi   TRIGANO     356.99   \n",
       "26260      Lit complet avec coffre Gris 140x190cm Cosmo    AUCUNE  281100.00   \n",
       "1569                     Adhésif PVC toile orange 75X33    AUCUNE       4.80   \n",
       "\n",
       "       Categorie3                                    Categorie3_Name  \n",
       "20189  1000014152                                 HISTOIRE DE FRANCE  \n",
       "6928   1000003662                              BOUILLOIRE ELECTRIQUE  \n",
       "3422   1000002072           BALANCOIRE - PORTIQUE - STATION DE JEUX   \n",
       "26260  1000015782  LIT COMPLET - MEUBLE ET SOMMIER - MEUBLE ET SO...  \n",
       "1569   1000001535              RUBAN ADHESIF - ADHESIF PALETTISATION  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 6)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat = train[train['Categorie3'] == 1000014152]\n",
    "cat.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un premier modèle Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, ngram_range = (1,1), stop_words=stopWords, tokenizer = token.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['all', 'anmoins', 'apr', 'but', 'cel', 'ch', 'cinquanti', 'cinqui', 'del', 'derri', 'deuxi', 'diff', 'dixi', 'douzi', 'except', 'fa', 'hol', 'huiti', 'malgr', 'mement', 'neuvi', 'nomm', 'ol', 'oll', 'onzi', 'particuli', 'pass', 'pi', 'plut', 'pr', 'premi', 'quatri', 'quelqu', 're', 'rement', 'rent', 'rente', 'rentes', 'rents', 'res', 'revoil', 'septi', 'sixi', 'sormais', 'taient', 'tais', 'tait', 'tat', 'tiez', 'tions', 'tr', 'tre', 'troisi', 'voil'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "# Un premier modele simple entrainée sur le Libelle de l'article\n",
    "feature = 'Libelle'\n",
    "target = 'Categorie3'\n",
    "\n",
    "X_train, y_train, X_test, y_test = vectorizeData(vectorizer, train, test, feature, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Scoring Using the trainAndScore Function we've built above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.6474661166764879\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.7019740718915733\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "101033"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explorer les autres champs a savoir Description, Libelle et Marque\n",
    "# pour cela, créer une colonne à texte à partir des 3 précédentes\n",
    "train['Texte'] = train['Description'] +', '+ train[\"Libelle\"] +', ' + train[\"Marque\"]\n",
    "test['Texte'] = test['Description'] +', '+ test[\"Libelle\"] +', ' + test[\"Marque\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Le centenaire de la Grande Guerre - Jean Huon, 14-18 La guerre totale, AUCUNE'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Texte'][20189]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Mini Manuel, Architecture de l'ordinateur, AUCUNE\""
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['Texte'][20514]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing NaN Values after the merging in the Texte Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ BEFORE REPLACING :  14  NAN VALUES\n",
      "------ AFTER :  0  NAN VALUES\n"
     ]
    }
   ],
   "source": [
    "replaceNaNValues(train, 'Texte')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ BEFORE REPLACING :  11  NAN VALUES\n",
      "------ AFTER :  0  NAN VALUES\n"
     ]
    }
   ],
   "source": [
    "replaceNaNValues(test, 'Texte')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(lowercase=True, stop_words=stopWords, ngram_range=(1,1), tokenizer = token.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python36\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['all', 'anmoins', 'apr', 'but', 'cel', 'ch', 'cinquanti', 'cinqui', 'del', 'derri', 'deuxi', 'diff', 'dixi', 'douzi', 'except', 'fa', 'hol', 'huiti', 'malgr', 'mement', 'neuvi', 'nomm', 'ol', 'oll', 'onzi', 'particuli', 'pass', 'pi', 'plut', 'pr', 'premi', 'quatri', 'quelqu', 're', 'rement', 'rent', 'rente', 'rentes', 'rents', 'res', 'revoil', 'septi', 'sixi', 'sormais', 'taient', 'tais', 'tait', 'tat', 'tiez', 'tions', 'tr', 'tre', 'troisi', 'voil'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    }
   ],
   "source": [
    "feature = 'Texte'\n",
    "target = 'Categorie3'\n",
    "\n",
    "X_train, y_train, X_test, y_test = vectorizeData(vectorizer, train, test, feature, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251090"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 2, ..., 2, 3, 2], dtype=int64)"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20362, 31168)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6788, 31168)"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.6978491455509723\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.7555981143193872\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF/IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En utilisant la Feature 'Libelle' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Libelle'\n",
    "target = 'Categorie3'\n",
    "\n",
    "X_train, y_train, X_test, y_test = vectorizeData(vectorizer, train, test, feature, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.6565998821449617\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.6903358868591633\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## En utilisant la Feature 'Texte' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer ce nouveau modèle sur le champ texte et faire les étapes précédentes jusqu'à la modélisation et l'accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Texte'\n",
    "target = 'Categorie3'\n",
    "\n",
    "X_train, y_train, X_test, y_test = vectorizeData(vectorizer, train, test, feature, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.7015321154979376\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.7482321744254566\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On reprends les étapes précédentes avec le modele Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Texte'\n",
    "target = 'Categorie3'\n",
    "\n",
    "X_train, y_train, X_test, y_test = vectorizeData(vectorizer, train, test, feature, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.6957866823806718\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.7457277548615203\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of characters (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On reprend toutes les étapes précédentes avec les Bag of characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(analyzer=u'char_wb',stop_words=stopWords, tokenizer = token.tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Texte'\n",
    "target = 'Categorie3'\n",
    "\n",
    "X_train, y_train, X_test, y_test = vectorizeData(vectorizer, train, test, feature, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.44063052445492046\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGDClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.22038892162639953\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "> il est claire que 1 caractère n'est pas très représentatif"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of characters (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pareil avec 3 characters\n",
    "vectorizer = CountVectorizer(analyzer=u'char_wb', ngram_range=(3,3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Texte'\n",
    "target = 'Categorie3'\n",
    "\n",
    "X_train, y_train, X_test, y_test = vectorizeData(vectorizer, train, test, feature, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.6882734236888627\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.7276075427224514\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of characters (5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pareil avec 5 characters\n",
    "vectorizer = CountVectorizer(analyzer=u'char_wb', ngram_range=(5,5),stop_words=stopWords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = 'Texte'\n",
    "target = 'Categorie3'\n",
    "\n",
    "X_train, y_train, X_test, y_test = vectorizeData(vectorizer, train, test, feature, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20189    Le centenaire de la Grande Guerre - Jean Huon,...\n",
       "6928     Bouilloire sans fil - Puissance:  2200 W - Cap...\n",
       "3422     LIVRAISON INCLUSE Portique métal/bois 2,30 m 3...\n",
       "26260    Lit complet composé d'1 cadre de lit, de 2 mat...\n",
       "1569     Adhésif PVC toile orange 75X33, Adhésif PVC to...\n",
       "Name: Texte, dtype: object"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['Texte'].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.7205362404242781\n"
     ]
    }
   ],
   "source": [
    "model = MultinomialNB()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGDClassifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORE =   0.7442545668827343\n"
     ]
    }
   ],
   "source": [
    "model = SGDClassifier()\n",
    "trainAndScore(model, X_train, y_train, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
